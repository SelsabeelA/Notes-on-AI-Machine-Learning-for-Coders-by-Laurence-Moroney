{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Notes on: AI & Machine Learning for Coders by Laurence Moroney**\n",
        "*Selsabeel A.*"
      ],
      "metadata": {
        "id": "FzUyKWqWD4in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n"
      ],
      "metadata": {
        "id": "y67reTwp7nhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Definition\n",
        "model = Sequential([Dense(units=1, input_shape=[1])])\n",
        "# Sequential: Linear stack of layers in the neural network\n",
        "# Dense: Fully connected layer with one neuron\n",
        "# units=1: One neuron in this dense layer\n",
        "# input_shape=[1]: Single input for this layer\n",
        "\n",
        "# Model Compilation\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "# optimizer='sgd': Stochastic Gradient Descent as the optimization algorithm\n",
        "# loss='mean_squared_error': Mean Squared Error as the loss function\n",
        "\n",
        "# Data Preparation\n",
        "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
        "# xs: Input data (independent variable)\n",
        "# ys: Output data (dependent variable)\n",
        "\n",
        "# Model Training\n",
        "model.fit(xs, ys, epochs=500)\n",
        "# model.fit: Trains the model on the input data\n",
        "# epochs=500: Model goes through the entire dataset 500 times during training"
      ],
      "metadata": {
        "id": "gbNxyqFV7to9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "print(model.predict([10.0]))\n",
        "# Use the trained model to predict the output for the input value 10.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCtmZVo9Ak31",
        "outputId": "b38f39fb-a9e8-468a-81a8-0fbe6be70e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 112ms/step\n",
            "[[18.984798]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Flatten(input_shape=(28,28)),\n",
        "    # Flatten is not a layer. It's a specification for the input layer. We want our input\n",
        "    # to be treated as a series of numerial values instead of pixeled images.\n",
        "    # It essentially turns the 2D array of RGB values into a 1D array.\n",
        "    Dense(128, activation=tf.nn.relu),\n",
        "    # This is a layer of neurons from the hidden layer since it's not the input or output and \"hidden\"\n",
        "    # from us. We're asking for an arbitrary number, 128, of neurons to have their internal parameters\n",
        "    # randomly initialized using the RelU activation function.\n",
        "    # The more neurons we assign, the more parameters it'll learn. It'll also means the code will run more slowly.\n",
        "    # Too many neurons causes overfitting. Too few causes underfitting.\n",
        "    # the RelU activation function simply returns a value if it's greater than 0.\n",
        "    # It helps with trying to figure out a pattern ignoring the negative sign.\n",
        "    Dense(10, activation=tf.nn.softmax)\n",
        "    # This is the output layer. It contains 10 neurons since we want our output to be 10 classes (10 types of clothing items)\n",
        "    # The value of the output layer at each neuron is the probability that __ is the correct prediction.\n",
        "    # So neuron 4 will contain the probability that the image is type 4. Now we want to easily pick the\n",
        "    # highest probability and output it as our final prediction, which can easily be done using softmax\n",
        "    # activation function.\n",
        "])"
      ],
      "metadata": {
        "id": "6-CyGKj1BD3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entire code for the Fashion MNIST data"
      ],
      "metadata": {
        "id": "LZ9WYguzGhvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.95):\n",
        "      print(\"\\nReached 95% accuracy so let's cancel training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images = training_images / 255.0\n",
        "# we're turning the image from grayscale to RGB values 0~255. It's called\n",
        "# nornalizing the image. Normalizing works better since it doesn't\n",
        "# amplify the differences in big values and ignore differences in small values.\n",
        "test_images = test_images / 255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50, callbacks=[callbacks])\n",
        "# sparse *categorical* cross entropy is a good loss function to pick here since\n",
        "# the predictions we're making are choosing a *category*.\n",
        "# We want the accuracy to be printed as the model works."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT40uLvHGfW0",
        "outputId": "94db9896-650c-46a7-ab8f-874a4b46c14c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.4981 - accuracy: 0.8248\n",
            "Epoch 2/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3736 - accuracy: 0.8657\n",
            "Epoch 3/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3364 - accuracy: 0.8778\n",
            "Epoch 4/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3120 - accuracy: 0.8858\n",
            "Epoch 5/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2974 - accuracy: 0.8905\n",
            "Epoch 6/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2821 - accuracy: 0.8953\n",
            "Epoch 7/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2703 - accuracy: 0.8998\n",
            "Epoch 8/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2581 - accuracy: 0.9042\n",
            "Epoch 9/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2486 - accuracy: 0.9086\n",
            "Epoch 10/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2406 - accuracy: 0.9100\n",
            "Epoch 11/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2326 - accuracy: 0.9126\n",
            "Epoch 12/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2253 - accuracy: 0.9148\n",
            "Epoch 13/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2175 - accuracy: 0.9186\n",
            "Epoch 14/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2111 - accuracy: 0.9208\n",
            "Epoch 15/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2049 - accuracy: 0.9236\n",
            "Epoch 16/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1992 - accuracy: 0.9255\n",
            "Epoch 17/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1919 - accuracy: 0.9278\n",
            "Epoch 18/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1883 - accuracy: 0.9291\n",
            "Epoch 19/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1834 - accuracy: 0.9316\n",
            "Epoch 20/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1791 - accuracy: 0.9327\n",
            "Epoch 21/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1754 - accuracy: 0.9332\n",
            "Epoch 22/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1701 - accuracy: 0.9360\n",
            "Epoch 23/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1659 - accuracy: 0.9384\n",
            "Epoch 24/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1633 - accuracy: 0.9389\n",
            "Epoch 25/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1579 - accuracy: 0.9397\n",
            "Epoch 26/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1544 - accuracy: 0.9416\n",
            "Epoch 27/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1509 - accuracy: 0.9432\n",
            "Epoch 28/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1497 - accuracy: 0.9439\n",
            "Epoch 29/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1440 - accuracy: 0.9462\n",
            "Epoch 30/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1423 - accuracy: 0.9462\n",
            "Epoch 31/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1390 - accuracy: 0.9484\n",
            "Epoch 32/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1381 - accuracy: 0.9474\n",
            "Epoch 33/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1359 - accuracy: 0.9483\n",
            "Epoch 34/50\n",
            "1861/1875 [============================>.] - ETA: 0s - loss: 0.1313 - accuracy: 0.9509\n",
            "Reached 95% accuracy so let's cancel training!\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1310 - accuracy: 0.9510\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fd785ffc310>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_images, test_labels)\n",
        "# This code will test our model on the test data, compare if it's correct, to *evaluate* it."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikBLl-IlHFDe",
        "outputId": "f4f56851-2217-4b39-d9b9-9a21a82bb62c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3410 - accuracy: 0.8783\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3409726321697235, 0.8783000111579895]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifications = model.predict(test_images)\n",
        "print(classifications[0])\n",
        "print(test_labels[0])\n",
        "# Here for our code, it's essentially saying the probabilities that the item of clothing number #0\n",
        "# for each 10 categories is so, so, and so. We observe that the highest probaility is for category 9,\n",
        "# at 61.4%. The rest are very tiny to the power of -5+ or whatever."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "837AWUtxKQJd",
        "outputId": "47f73c2a-270e-4bae-d13a-ab92b50348d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step\n",
            "[1.9331756e-06 5.5637788e-08 3.2111581e-07 4.2911299e-08 2.0162581e-07\n",
            " 2.4268976e-01 4.4991248e-06 1.1581589e-01 1.9732064e-05 6.4146751e-01]\n",
            "9\n"
          ]
        }
      ]
    }
  ]
}